{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Basics Overview\n",
    "\n",
    "- Tutorial: [langgraph basics](https://langchain-ai.github.io/langgraph/tutorials/introduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, getpass\n",
    "from anthropic import Anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "#Import API Keys        \n",
    "_root = \"/home/zjc1002/Mounts/code/admin/\"\n",
    "sys.path.append(_root)\n",
    "from api_keys import _api_keys\n",
    "\n",
    "#set Environment Variables \n",
    "def _set_env(var: str,value: str = None):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = value\n",
    "        #getpass.getpass(f\"{var}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: claude-3-7-sonnet-20250219\n",
      "Model ID: claude-3-5-sonnet-20241022\n",
      "Model ID: claude-3-5-haiku-20241022\n",
      "Model ID: claude-3-5-sonnet-20240620\n",
      "Model ID: claude-3-haiku-20240307\n",
      "Model ID: claude-3-opus-20240229\n",
      "Model ID: claude-3-sonnet-20240229\n",
      "Model ID: claude-2.1\n",
      "Model ID: claude-2.0\n"
     ]
    }
   ],
   "source": [
    "#SET ANTHROPIC API KEY ENV VAR\n",
    "_set_env(\"ANTHROPIC_API_KEY\"\n",
    "         ,value =  _api_keys['ANTHROPIC_API_KEY'])\n",
    "\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "client = Anthropic(api_key=_api_keys['ANTHROPIC_API_KEY'])\n",
    "\n",
    "# Get available models\n",
    "available_models = client.models.list()\n",
    "\n",
    "# Print the available models\n",
    "for model in available_models.data:\n",
    "    print(f\"Model ID: {model.id}\")\n",
    "\n",
    "    # The context_window property contains the maximum number of tokens\n",
    "    if hasattr(model, 'context_window'):\n",
    "        print(f\"Max context length: {model.context_window} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select LLM TO  use in chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model to use in chatbot (use tiny model that is cheap)\n",
    "_model_id = \"claude-3-haiku-20240307\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Build simple chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Create a ***StateGraph***. A StateGraph object defines the structure of our chatbot as a \"state machine\". \n",
    "  - **Nodes**  represent the llm and functions our chatbot can call \n",
    "    - *Nodes represent units of work, they are typically regular python functions*\n",
    "  - **Edges**  specify how the bot should transition between these functions.\n",
    "\n",
    "\n",
    "***Note:*** *the Graph State includes the graphs SCHEMA and REDUCER FUNCTIONS  which handel state updates.* **add_messages()** is the reducer function used in this example to append new messages to the list instead of replacing them. Keys without a reducer annotation will overwrite previous values  \n",
    "\n",
    "##### B. Add a chatbot Node to StateGraph()\n",
    " - The **chatbot node** function takes the current State as input and returns a dictionary containing an updated messages list under the key \"messages\". **This is the basic pattern for all LangGraph node functions.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # in this case we use the reducer function add_messages to  append messages to the list, rather than overwriting them\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "#define config schemea to use to pass configurable parameters in your API \n",
    "#as example i add a string paramter that will be prefixed to each message (not a good idea, but illistrates how to use config)\n",
    "class ConifgSchema(TypedDict):\n",
    "    prefix: str \n",
    "\n",
    "\n",
    "#basic state\n",
    "# The state is a dictionary that contains the current state of the graph\n",
    "# the config contains paramters you want to pass to the node operations\n",
    "graph_builder = StateGraph(State, config_schema=ConifgSchema)\n",
    "\n",
    "\n",
    "### Add a chatbot Node to the graph\n",
    "llm = ChatAnthropic(model=_model_id)\n",
    "\n",
    "# Note: the chatbot node function takes the current State as input and returns a dictionary containing an updated messages list under the key \"messages\". **This is the basic pattern for all LangGraph node functions.**\n",
    "def chatbot(state: State, config: RunnableConfig)-> dict:\n",
    "    prefix = config['configurable'].get(\"prefix\",\"\")\n",
    "    _message = llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    return {\"messages\": [_message]}\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "\n",
    "#Next, add an entry point. This tells our graph where to start its work each time we run it.\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "#set an exit point. This tells the graph \"any time this node is run, you can exit.\"\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Finally, we compile the graph. This will check for any errors in the graph and prepare it for use. This creates a new class that we can use to run the graph.\n",
    "# The compiled graph is a subclass of the original graph builder, so we can still use the same methods.\n",
    "# The compiled graph will have a new name, so we can use it to run the graph.\n",
    "# This will also create a new class that we can use to run the graph.\n",
    "graph = graph_builder.compile() \n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm afraid I don't have enough context to determine who \"Mike\" is without more information. \"Mike\" could refer to a wide variety of people, as it is a common first name. Can you provide some additional details about who you are asking about? That would help me try to understand who the specific \"Mike\" you are referring to is.\n",
      "dict_keys(['id', 'model', 'stop_reason', 'stop_sequence', 'usage', 'model_name'])\n",
      "Message ID: msg_016ZjLXK7vY4Fpk4YcgLp576\n",
      "Model Used: claude-3-haiku-20240307\n",
      "Chat Stop Reason: end_turn\n",
      "Chat Stop Sequence: None\n",
      "Chat usage: {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 11, 'output_tokens': 75}\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        #import pdb;pdb.set_trace();\n",
    "        \n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "            print(value[\"messages\"][-1].response_metadata.keys())\n",
    "            print(f\"Message ID: {value['messages'][-1].response_metadata['id']}\")\n",
    "            print(f\"Model Used: {value['messages'][-1].response_metadata['model']}\")\n",
    "            print(f\"Chat Stop Reason: {value['messages'][-1].response_metadata['stop_reason']}\")\n",
    "            print(f\"Chat Stop Sequence: {value['messages'][-1].response_metadata['stop_sequence']}\")\n",
    "            print(f\"Chat usage: {value['messages'][-1].response_metadata['usage']}\")\n",
    "\n",
    "# This is a simple loop to get user input and stream the graph updates\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Enhance Chatbot with Tools**\n",
    " 1. Specify tavily api key (tavily is used to search internet like google but designed for llms)\n",
    " 2. Define the tool TavilySearchResults to inovke \n",
    " 3. Rebuild the chatbot from parrt one with **bind_tools** added to the llm. (this tells llm how to format json get/post requests when using the tavily search api)\n",
    " 4. Add tools to a new node in the **STATEGRAPH** to create function to run tools if called \n",
    "    - ***BasicToolNode*** is a class that checks the most recent message in the ***STATE*** instance and calls tools if the message contains **tool_calls**. Langchain provides a prebuilt ***ToolNode*** class to abstract this if building from scratch is to painful\n",
    " 5. Define ***conditional edges*** to route the contorl flow from one note to another. *Conditional Edges* usually ocntain  'if' statements to route to different nodes depending on the current graph state.  These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n",
    "    - Note: the prebuilt *tools_condition()* method should be used in production , we define our tool router manually for instructive purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>THIS IS A TEST TO SHOW FORMAT OF TAVILY RESPONSE:</b> \n",
      "\n",
      "[{'title': 'Zachary Carideo - Lead Data Scientist (Machine Learning Center of ...', 'url': 'https://www.zoominfo.com/p/Zachary-Carideo/-2145059615', 'content': 'Zachary Carideo is a Lead Data Scientist (Machine Learning Center of Excellence) at Wells Fargo based in San Francisco, California. Previously, Zachary was', 'score': 0.8812988}, {'title': 'Zachary Carideo Email & Phone Number | Wells Fargo - ContactOut', 'url': 'https://contactout.com/Zachary-Carideo-27251321', 'content': \"What is Zachary Carideo's role at Wells Fargo? Zachary Carideo is Lead Data Scientist (Machine Learning Center of Excellence). What is Zachary Carideo's Phone\", 'score': 0.8548001}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# This tool requires an API key to be set in the environment variable TAVILY_API_KEY\n",
    "_set_env(\"TAVILY_API_KEY\"\n",
    "         ,value =  _api_keys['TAVILY_API_KEY'])\n",
    "\n",
    "#DEFINE THE TOOL \n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "\n",
    "# The tool will return a dictionary with the results\n",
    "print(f\"<b>THIS IS A TEST TO SHOW FORMAT OF TAVILY RESPONSE:</b> \\n\\n{tool.invoke('who is zachary carideo?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f0db3cb9150>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "from langchain_core.messages import ToolMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "#DEFINE THE GRAPH \n",
    "\n",
    "#SAME AS PT 1\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "class ConifgSchema(TypedDict):\n",
    "    use_tavily: bool \n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "llm = ChatAnthropic(model=_model_id)\n",
    "\n",
    "\n",
    "# NEW TO PART2\n",
    "# Modification: tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "# Note: the chatbot node function takes the current State as input and returns a dictionary containing \n",
    "# an updated messages list under the key \"messages\". **This is the basic pattern for all LangGraph node functions.\n",
    "# NOTE: here you  can use the  config to pass parameters into the tools you are calling (here Tavily Search parameters)\n",
    "def chatbot(state: State, config: RunnableConfig)-> dict:\n",
    "    _use_internet = config['configurable'].get(\"use_tavily\",False)\n",
    "    _message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [_message]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "\n",
    "# Add a BasicToolNode that checks the most recent message in the state and calls tools if the message contains tool_calls. \n",
    "# It relies on the LLM's tool_calling support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers.\n",
    "class BasicToolNode: \n",
    "\n",
    "    \"\"\"A node that runs the tools requested in the  last AIMessage\"\"\"\n",
    "\n",
    "    def __init__(self\n",
    "                 ,tools: list\n",
    "                 ) -> None: \n",
    "        \n",
    "        self.tools_by_name = {t.name: t for t in tools}\n",
    "    \n",
    "    def __call__(self,inputs: dict): \n",
    "\n",
    "        # Check the most recent message in the state and call tools if the message contains tool_calls\n",
    "        # check if the last message is a tool call\n",
    "        # if the last message is a tool call, run the tools\n",
    "        # if the last message is not a tool call, return the inputs\n",
    "\n",
    "        #Note: in python empty collections(like []) are considered False, while non-empty collections are considered True\n",
    "        if messages := inputs.get(\"messages\",[]): \n",
    "            message = messages[-1]\n",
    "            print(isinstance(message, ToolMessage))\n",
    "        else: \n",
    "            raise ValueError('No Messages Found in inputs')\n",
    "\n",
    "\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls: \n",
    "            \n",
    "            #import pdb;pdb.set_trace();\n",
    "            #run the tool and get the result\n",
    "            tool_result = self.tools_by_name[tool_call['name']].invoke(tool_call['args']\n",
    "                                                                       )\n",
    "            #add the result to the outputs list\n",
    "            outputs.append(\n",
    "                ToolMessage(content=json.dumps(tool_result),\n",
    "                            name = tool_call['name'], \n",
    "                            tool_call_id = tool_call[\"id\"]\n",
    "                            )\n",
    "            )\n",
    "\n",
    "        return {'messages': outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm afraid I don't have enough context to provide a meaningful response to just \"hello\". As an AI assistant, I'm designed to have natural conversations and provide helpful information to users. Could you please rephrase your request or provide more details about what you'd like assistance with? I'll be happy to try my best to help once I understand the specifics of your needs.\n",
      "dict_keys(['id', 'model', 'stop_reason', 'stop_sequence', 'usage', 'model_name'])\n",
      "Message ID: msg_01UiRVJW3nPC7ga1s9DZSVVX\n",
      "Model Used: claude-3-haiku-20240307\n",
      "Chat Stop Reason: end_turn\n",
      "Chat Stop Sequence: None\n",
      "Chat usage: {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 368, 'output_tokens': 81}\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "\n",
    "    #get the message if it exists\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    \n",
    "    #If message exists check if a tool has been referenced \n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\"\n",
    "     , END: END},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "#RUN IT \n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
